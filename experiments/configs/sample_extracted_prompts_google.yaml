# Configuration for sampling LLM responses via Google Gemini API
# Experiment: Sample responses from extracted WildChat prompts

# Dataset configuration
dataset:
  source: "json"                  # Use JSON file as source
  json_path: "extracted_prompts.json"  # Path to JSON file with prompts (array of strings)
  num_prompts: null                 # Number of prompts to sample
  seed: 42                        # Random seed for reproducibility
  sampling_mode: "random"         # How to sample prompts: "first" (take first N) or "random" (random N)

  # Prompt extraction
  # prompt_field is not needed when JSON is a direct array of strings

  # Optional filters
  min_length: 1                   # Minimum prompt length in characters
  max_length: null                # Maximum prompt length in characters (null for no limit)

# Models to sample from (list of arbitrary length)
# Model IDs from Google Gemini: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models
models:
  - "gemini-2.5-flash-lite-preview-09-2025"
  # - "gemini-2.5-flash-preview-04-2025"
  # - "gemini-2.5-pro-preview-03-2025"
  # Add more models as needed

# API configuration
api:
  project_id: null                # Google Cloud project ID (if null, uses GOOGLE_CLOUD_PROJECT env var)
  location: "global"              # Google Cloud region (default: "global")

# Sampling parameters
sampling:
  num_samples_per_prompt: 5       # Number of samples per prompt per model
  max_tokens: 4096                # Maximum tokens to generate
  temperature: 1.0                # Sampling temperature
  top_p: 1.0                      # Nucleus sampling parameter
  response_logprobs: true         # Whether to return log probabilities
  logprobs: 20                    # Number of top logprobs to return per token (1-20)
  max_retries: 5                  # Maximum retry attempts for failed requests

# Output configuration
output:
  base_dir: "experiments/results_final"
  experiment_name: "responses_gemini-2.5-flash-lite-preview-09-2025"
