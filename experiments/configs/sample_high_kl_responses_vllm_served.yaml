# Configuration for sampling LLM responses for high KL divergence tokens
# Takes high_kl_tokens.json as input and samples multiple responses for each entry

# Model to sample from (single model)
model: "unsloth/Llama-3.3-70B-Instruct-bnb-4bit"

# API configuration
api:
  base_url: "http://0.0.0.0:8000/v1"

# Entry selection
entries:
  max_entries: null          # Maximum number of entries to process (null = all non-infinite entries)

# Sampling parameters
sampling:
  num_samples_per_entry: 20  # Number of samples per high KL entry
  max_tokens: 20            # Maximum tokens to generate per response
  temperature: 1.0           # Sampling temperature
  top_p: 1.0                 # Nucleus sampling parameter
  logprobs: false             # Whether to return log probabilities
  top_logprobs: 5            # Number of top logprobs to return per token
  reasoning: false           # Whether to enable reasoning
  max_retries: 5             # Maximum retry attempts for failed requests

# Output configuration
output:
  base_dir: "experiments/results/llama-3-70b-diff"
  experiment_name: "high_kl_responses-3.3_v2"
