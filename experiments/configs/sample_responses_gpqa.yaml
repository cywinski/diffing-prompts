# Configuration for sampling LLM responses on GPQA-Diamond dataset
# GPQA-Diamond: Graduate-level science questions across multiple domains

# Dataset configuration
dataset:
  source: "huggingface"           # Options: "huggingface", "file"
  dataset_name: "fingertap/GPQA-Diamond"  # GPQA-Diamond dataset
  split: "test"                   # Dataset split (GPQA-Diamond has only "test")
  num_prompts: 198                # Number of prompts to sample (max 198 for full dataset)
  seed: 42                        # Random seed for reproducibility
  sampling_mode: "first"          # How to sample prompts: "first" (take first N) or "random" (random N)

  # Prompt extraction
  use_gpqa_format: true           # Use GPQA extractor that adds answer tag instructions
  # The extractor will append: "Please provide your answer between <answer> tags, like <answer>A</answer>."

  # Optional filters
  # min_length: 50                # Minimum prompt length in characters
  # max_length: 5000              # Maximum prompt length in characters

# Models to sample from (list of arbitrary length)
# Model IDs from OpenRouter: https://openrouter.ai/models
models:
  - "meta-llama/llama-3.3-70b-instruct"
  # - "deepseek/deepseek-chat"
  # Add more models as needed

# API configuration
api:
  base_url: "https://openrouter.ai/api/v1"  # OpenRouter API base URL

# Sampling parameters
sampling:
  num_samples_per_prompt: 5      # Number of samples per prompt per model
  max_tokens: 4096               # Maximum tokens to generate
  temperature: 1.0               # Sampling temperature
  top_p: 1.0                     # Nucleus sampling parameter
  logprobs: false                # Whether to return log probabilities
  top_logprobs: 0                # Number of top logprobs to return per token
  reasoning: false               # Whether to enable reasoning
  max_retries: 5                 # Maximum retry attempts for failed requests

# Output configuration
output:
  base_dir: "experiments/results"
  experiment_name: "gpqa_samples"
