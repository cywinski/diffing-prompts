# Configuration for calculating KL divergence between two models
# First model (base model) - P distribution
model_2_name: "unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit"

# Second model (fine-tuned/modified model) - Q distribution
model_1_name: "unsloth/Llama-3.3-70B-Instruct-bnb-4bit"
tokenizer_1_name: "unsloth/Llama-3.3-70B-Instruct-bnb-4bit"
tokenizer_2_name: "unsloth/Llama-3.3-70B-Instruct-bnb-4bit"

# Directory containing response files from model 2
# These responses will be used to calculate KL divergence
responses_dir_model2: "/workspace/projects/diffing-prompts/experiments/results/openrouter_samples/llama-3.3-70b"

# Output directory for KL divergence results
output_dir: "/workspace/projects/diffing-prompts/experiments/results/kl/kl_llama-3.3-70b_llama-3.1-70b"

# Intermediate directory for two-stage processing
# Used to save model_1 log probabilities when running stage1
# If not specified, defaults to {output_dir}/intermediate
intermediate_dir: "/workspace/projects/diffing-prompts/experiments/results/kl/kl_llama-3.3-70b_llama-3.1-70b/intermediate"

# Device mapping strategy for model loading
# Options: "auto", "cuda:0", "cpu", etc.
device_map: "auto"

# Reasoning trace configuration
# Set to true to include reasoning traces in KL divergence calculation
# Reasoning will be extracted from the "reasoning" field in response JSON if present
include_reasoning: false

# Thinking tokens to wrap reasoning traces
# Default values work for models like DeepSeek-R1
thinking_token_start: "<think>"
thinking_token_end: "</think>"
